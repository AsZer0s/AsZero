---
title: æœ¬åœ°éƒ¨ç½²DeepSeek-R1å¤§æ¨¡å‹ï¼å…è´¹å¼€æºï¼Œåª²ç¾OpenAI-o1èƒ½åŠ›
date: 2025-01-26 10:44:20
tags: 
   - å¤§æ¨¡å‹
   - DeepSeek
---

æœ€è¿‘ï¼Œä¸€å®¶åå«DeepSeekçš„åˆåˆ›å…¬å¸ç»è¿‡æŠ€æœ¯è¿­ä»£ä¸å‡çº§ï¼Œå‘å¸ƒäº†å…¨æ–°ä¸€ä»£å¤§æ¨¡å‹ï¼Œâ€œDeepSeek-V3â€

ç”±äºè¿™æ¬¾å¤§æ¨¡å‹å¤ªè¿‡å¥½ç”¨ï¼ŒDeepSeek R1 æ›´æ˜¯ç›´æ¥å…è´¹å¼€æºï¼Œåœ¨AIå‘çƒ§å‹åœˆå­ä¼ æ’­åï¼Œä¼ åˆ°äº†æµ·å¤–ç¤¾äº¤å¹³å°ã€æŠ€æœ¯è®ºå›ï¼Œå¼•å‘äº†æµ·å¤–ç½‘å‹çš„è¿è¿ç§°èµ

![DeepSeek Logo](https://s2.loli.net/2025/01/26/ZMcrqlg2YuPEX8Q.png)

å„é¡¹æ€§èƒ½æŒ‡æ ‡æ›´æ˜¯å’ŒOpenAI-o1 æ¨¡å‹ä¸ç›¸ä¸Šä¸‹ï¼Œç”šè‡³åšåˆ°äº†å°éƒ¨åˆ†çš„è¶…è¶Šï¼Œå…³é”®æ˜¯å¼€æºçš„ï¼Œæˆ‘ä»¬å¯ä»¥æœ¬åœ°éƒ¨ç½²ä½¿ç”¨

![DeepSeekå¯¹æ¯”](https://s2.loli.net/2025/01/26/lHLGOCd1cjJv6oT.png)

## 1ã€æœ¬åœ°éƒ¨ç½²ï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡Ollamaæ¥è¿›è¡Œå®‰è£…
Ollama å®˜æ–¹ç‰ˆï¼šã€[ç‚¹å‡»å‰å¾€](https://ollama.com/)ã€‘
Web UI æ§åˆ¶ç«¯ã€[ç‚¹å‡»å®‰è£…](https://chromewebstore.google.com/detail/page-assist-%E6%9C%AC%E5%9C%B0-ai-%E6%A8%A1%E5%9E%8B%E7%9A%84-web/jfgfiigpkhlkbnfnbobbkinehhfdhndo)ã€‘

## å®‰è£…å‘½ä»¤

### Ollamaæ¨¡å‹
1.5B Qwen DeepSeek R1
```bash
ollama run deepseek-r1:1.5b
```

7B Qwen DeepSeek R1
```bash
ollama run deepseek-r1:7b
```

8B Llama DeepSeek R1
```bash
ollama run deepseek-r1:8b
```

14B Qwen DeepSeek R1
```bash
ollama run deepseek-r1:14b
```

32B Qwen DeepSeek R1
```bash
ollama run deepseek-r1:32b
```

70B Llama DeepSeek R1
```bash
ollama run deepseek-r1:70b
```

### æ›´å¤šæ¨¡å‹ä¸‹è½½

DeepSeek-R1
|       æ¨¡å‹       	| #æ€»å‚æ•° 	| #å·²æ¿€æ´»å‚æ•° 	| ä¸Šä¸‹æ–‡é•¿åº¦ 	|      ä¸‹è½½     	|
|:----------------:	|:-------:	|:-----------:	|:----------:	|:-------------:	|
| DeepSeek-R1-Zero 	| 671B    	| 37B         	| 128åƒ      	| ğŸ¤— [HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1-Zero) 	|
| DeepSeek-R1      	| 671B    	| 37B         	| 128åƒ      	| ğŸ¤— [HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1) 	|
DeepSeek-R1-Zero å’Œ DeepSeek-R1 åŸºäº DeepSeek-V3-Base è¿›è¡Œè®­ç»ƒ
æœ‰å…³æ¨¡å‹æ¶æ„çš„æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…DeepSeek-V3å­˜å‚¨åº“

DeepSeek-R1-Distill æ¨¡å‹
|              æ¨¡å‹             	|        åŸºç¡€æ¨¡å‹        	|      ä¸‹è½½     	|
|:-----------------------------:	|:----------------------:	|:-------------:	|
| DeepSeek-R1-Distill-Qwen-1.5B 	| [Qwen2.5-Math-1.5B](https://huggingface.co/Qwen/Qwen2.5-Math-1.5B)      	| ğŸ¤— [HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B) 	|
| DeepSeek-R1-Distill-Qwen-7B   	| [Qwen2.5-Math-7B](https://huggingface.co/Qwen/Qwen2.5-Math-7B)        	| ğŸ¤— [HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-7B) 	|
| DeepSeek-R1-Distill-Llama-8B  	| [Llama-3.1-8B](https://huggingface.co/meta-llama/Llama-3.1-8B)           	| ğŸ¤— [HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Llama-8B) 	|
| DeepSeek-R1-Distill-Qwen-14B  	| [Qwen2.5-14B](https://huggingface.co/Qwen/Qwen2.5-14B)            	| ğŸ¤— [HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-14B) 	|
| DeepSeek-R1-Distill-Qwen-32B  	| [Qwen2.5-32B](https://huggingface.co/Qwen/Qwen2.5-32B)            	| ğŸ¤— [HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-32B) 	|
| DeepSeek-R1-Distill-Llama-70B 	| [Llama-3.3-70B-Instruct](https://huggingface.co/meta-llama/Llama-3.3-70B-Instruct) 	| ğŸ¤— [HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Llama-70B) 	|
DeepSeek-R1-Distill æ¨¡å‹åŸºäºå¼€æºæ¨¡å‹è¿›è¡Œäº†å¾®è°ƒï¼Œä½¿ç”¨äº† DeepSeek-R1 ç”Ÿæˆçš„æ ·æœ¬
æˆ‘ä»¬å¯¹å…¶é…ç½®å’Œåˆ†è¯å™¨è¿›è¡Œäº†è½»å¾®æ›´æ”¹
è¯·ä½¿ç”¨æˆ‘ä»¬çš„è®¾ç½®æ¥è¿è¡Œè¿™äº›æ¨¡å‹

## è¯„ä¼°ç»“æœ
### DeepSeek-R1-è¯„ä¼°
å¯¹äºæˆ‘ä»¬æ‰€æœ‰çš„æ¨¡å‹ï¼Œæœ€å¤§ç”Ÿæˆé•¿åº¦è®¾ç½®ä¸º 32,768 ä¸ª token
å¯¹äºéœ€è¦é‡‡æ ·çš„åŸºå‡†ï¼Œæˆ‘ä»¬ä½¿ç”¨çš„æ¸©åº¦ä¸º0.6ï¼Œtop-p å€¼ä¸º0.95ï¼Œå¹¶ä¸ºæ¯ä¸ªæŸ¥è¯¢ç”Ÿæˆ 64 ä¸ªå“åº”æ¥ä¼°è®¡ pass@1
| ç±»åˆ« 	|        åŸºå‡†ï¼ˆå…¬åˆ¶ï¼‰        	| Claude 3.5 Sonnet-1022 	| GPT-4o 0513 	| DeepSeek V3 	| OpenAI o1-mini 	| OpenAI o1-1217 	| DeepSeek R1 	|
|:----:	|:--------------------------:	|:------------------------:	|:-----------:	|:-----------:	|:--------------:	|:--------------:	|:-----------:	|
|      	| å»ºç­‘å­¦                     	| â€“                        	| â€“           	| æ•™è‚²éƒ¨      	| â€“              	| â€“              	| æ•™è‚²éƒ¨      	|
|      	| # æ¿€æ´»å‚æ•°                 	| â€“                        	| â€“           	| 37B         	| â€“              	| â€“              	| 37B         	|
|      	| # æ€»å‚æ•°                   	| â€“                        	| â€“           	| 671B        	| â€“              	| â€“              	| 671B        	|
| è‹±è¯­ 	| MMLUï¼ˆé€šè¿‡@1ï¼‰             	| 88.3                     	| 87.2        	| 88.5        	| 85.2           	| 91.8           	| 90.8        	|
|      	| MMLU-Reduxï¼ˆEMï¼‰           	| 88.9                     	| 88.0        	| 89.1        	| 86.7           	| â€“              	| 92.9        	|
|      	| MMLU-Proï¼ˆEMï¼‰             	| 78.0                     	| 72.6        	| 75.9        	| 80.3           	| â€“              	| 84.0        	|
|      	| æ‰è½ (3 å‘ F1)             	| 88.3                     	| 83.7        	| 91.6        	| 83.9           	| 90.2           	| 92.2        	|
|      	| IF-Evalï¼ˆæç¤ºä¸¥æ ¼ï¼‰        	| 86.5                     	| 84.3        	| 86.1        	| 84.8           	| â€“              	| 83.3        	|
|      	| GPQA-é’»çŸ³çº§ (Pass@1)       	| 65.0                     	| 49.9        	| 59.1        	| 60.0           	| 75.7           	| 71.5        	|
|      	| SimpleQAï¼ˆæ­£ç¡®ï¼‰           	| 28.4                     	| 38.2        	| 24.9        	| 7.0            	| 47.0           	| 30.1        	|
|      	| æ¡†æ¶ï¼ˆé…ä»¶ï¼‰               	| 72.5                     	| 80.5        	| 73.3        	| 76.9           	| â€“              	| 82.5        	|
|      	| AlpacaEval2.0 (LC-èƒœç‡)    	| 52.0                     	| 51.1        	| 70.0        	| 57.8           	| â€“              	| 87.6        	|
|      	| ArenaHardï¼ˆGPT-4-1106ï¼‰    	| 85.2                     	| 80.4        	| 85.5        	| 92.0           	| â€“              	| 92.3        	|
| ä»£ç  	| LiveCodeBench (Pass@1-COT) 	| 33.8                     	| 34.2        	| â€“           	| 53.8           	| 63.4           	| 65.9        	|
|      	| Codeforcesï¼ˆç™¾åˆ†ä½æ•°ï¼‰     	| 20.3                     	| 23.6        	| 58.7        	| 93.4           	| 96.6           	| 96.3        	|
|      	| Codeforcesï¼ˆè¯„çº§ï¼‰         	| 717                      	| 759         	| 1134        	| 1820           	| 2061           	| 2029        	|
|      	| SWE å·²éªŒè¯ï¼ˆå·²è§£å†³ï¼‰       	| 50.8                     	| 38.8        	| 42.0        	| 41.6           	| 48.9           	| 49.2        	|
|      	| Aider-Polyglot (Acc.)      	| 45.3                     	| 16.0        	| 49.6        	| 32.9           	| 61.7           	| 53.3        	|
| æ•°å­¦ 	| AIME 2024ï¼ˆé€šè¡Œè¯@1ï¼‰      	| 16.0                     	| 9.3         	| 39.2        	| 63.6           	| 79.2           	| 79.8        	|
|      	| æ•°å­¦-500 (é€šè¿‡@1)          	| 78.3                     	| 74.6        	| 90.2        	| 90.0           	| 96.4           	| 97.3        	|
|      	| CNMO 2024 (é€šè¡Œè¯@1)       	| 13.1                     	| 10.8        	| 43.2        	| 67.6           	| â€“              	| 78.8        	|
| ä¸­æ–‡ 	| CLUEWSCï¼ˆEMï¼‰              	| 85.4                     	| 87.9        	| 90.9        	| 89.9           	| â€“              	| 92.8        	|
|      	| C-è¯„ä¼°ï¼ˆEMï¼‰               	| 76.7                     	| 76.0        	| 86.5        	| 68.9           	| â€“              	| 91.8        	|
|      	| C-SimpleQAï¼ˆæ­£ç¡®ï¼‰         	| 55.4                     	| 58.7        	| 68.0        	| 40.3           	| â€“              	| 63.7        	|

### è’¸é¦æ¨¡å‹è¯„ä¼°
|              æ¨¡å‹             	| AIME 2024 é€šè¡Œè¯@1 	| AIME 2024 ç¼ºç‚¹@64 	| MATH-500 é€šè¿‡@1 	| GPQA é’»çŸ³é€šè¡Œè¯@1 	| LiveCodeBench é€šè¡Œè¯@1 	| CodeForces è¯„çº§ 	|
|:-----------------------------:	|:------------------:	|:-----------------:	|:---------------:	|:-----------------:	|:----------------------:	|:---------------:	|
| GPT-4o-0513                   	| 9.3                	| 13.4              	| 74.6            	| 49.9              	| 32.9                   	| 759             	|
| å…‹åŠ³å¾·-3.5-åå››è¡Œè¯—-1022      	| 16.0               	| 26.7              	| 78.3            	| 65.0              	| 38.9                   	| 717             	|
| o1-è¿·ä½                        	| 63.6               	| 80.0              	| 90.0            	| 60.0              	| 53.8                   	| 1820            	|
| QwQ-32B-é¢„è§ˆ                  	| 44.0               	| 60.0              	| 90.6            	| 54.5              	| 41.9                   	| 1316            	|
| DeepSeek-R1-Distill-Qwen-1.5B 	| 28.9               	| 52.7              	| 83.9            	| 33.8              	| 16.9                   	| 954             	|
| DeepSeek-R1-Distill-Qwen-7B   	| 55.5               	| 83.3              	| 92.8            	| 49.1              	| 37.6                   	| 1189            	|
| DeepSeek-R1-Distill-Qwen-14B  	| 69.7               	| 80.0              	| 93.9            	| 59.1              	| 53.1                   	| 1481            	|
| DeepSeek-R1-Distill-Qwen-32B  	| 72.6               	| 83.3              	| 94.3            	| 62.1              	| 57.2                   	| 1691            	|
| DeepSeek-R1-Distill-Llama-8B  	| 50.4               	| 80.0              	| 89.1            	| 49.0              	| 39.6                   	| 1205            	|
| DeepSeek-R1-Distill-Llama-70B 	| 70.0               	| 86.7              	| 94.5            	| 65.2              	| 57.5                   	| 1633            	|
